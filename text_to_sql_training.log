2025-06-22 23:19:10,898 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-22 23:19:10,898 - INFO - Using base model: Qwen2.5-Coder:1.5B
2025-06-22 23:19:10,903 - INFO - ==================================================
2025-06-22 23:19:10,903 - INFO - TRAINING PHASE
2025-06-22 23:19:10,903 - INFO - ==================================================
2025-06-22 23:19:10,903 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-22 23:19:20,522 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-22 23:19:20,525 - INFO - Limited dataset to 1000 samples
2025-06-22 23:19:20,527 - INFO - Formatting dataset samples...
2025-06-22 23:19:20,646 - INFO - Successfully formatted 1000 samples
2025-06-22 23:19:20,655 - INFO - Used stratified split for balanced complexity distribution
2025-06-22 23:19:20,675 - INFO - Train samples: 900, Validation samples: 100
2025-06-22 23:19:20,841 - INFO - Processed datasets saved to ./processed_data/
2025-06-22 23:19:20,854 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-22 23:29:25,428 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-22 23:29:25,428 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-22 23:29:25,428 - INFO - ==================================================
2025-06-22 23:29:25,428 - INFO - TRAINING PHASE
2025-06-22 23:29:25,428 - INFO - ==================================================
2025-06-22 23:29:25,428 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-22 23:29:30,249 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-22 23:29:30,249 - INFO - Limited dataset to 1000 samples
2025-06-22 23:29:30,255 - INFO - Formatting dataset samples...
2025-06-22 23:29:30,343 - INFO - Successfully formatted 1000 samples
2025-06-22 23:29:30,353 - INFO - Used stratified split for balanced complexity distribution
2025-06-22 23:29:30,359 - INFO - Train samples: 900, Validation samples: 100
2025-06-22 23:29:30,469 - INFO - Processed datasets saved to ./processed_data/
2025-06-22 23:29:30,488 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-22 23:31:12,878 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-22 23:31:12,878 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-22 23:31:12,878 - INFO - ==================================================
2025-06-22 23:31:12,878 - INFO - TRAINING PHASE
2025-06-22 23:31:12,878 - INFO - ==================================================
2025-06-22 23:31:12,878 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-22 23:31:17,525 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-22 23:31:17,527 - INFO - Limited dataset to 1000 samples
2025-06-22 23:31:17,527 - INFO - Formatting dataset samples...
2025-06-22 23:31:17,629 - INFO - Successfully formatted 1000 samples
2025-06-22 23:31:17,629 - INFO - Used stratified split for balanced complexity distribution
2025-06-22 23:31:17,634 - INFO - Train samples: 900, Validation samples: 100
2025-06-22 23:31:17,777 - INFO - Processed datasets saved to ./processed_data/
2025-06-22 23:31:17,777 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-22 23:31:55,117 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-22 23:31:55,117 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-22 23:31:55,117 - INFO - ==================================================
2025-06-22 23:31:55,117 - INFO - TRAINING PHASE
2025-06-22 23:31:55,117 - INFO - ==================================================
2025-06-22 23:31:55,117 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-22 23:32:01,092 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-22 23:32:01,093 - INFO - Limited dataset to 1000 samples
2025-06-22 23:32:01,093 - INFO - Formatting dataset samples...
2025-06-22 23:32:01,187 - INFO - Successfully formatted 1000 samples
2025-06-22 23:32:01,189 - INFO - Used stratified split for balanced complexity distribution
2025-06-22 23:32:01,205 - INFO - Train samples: 900, Validation samples: 100
2025-06-22 23:32:01,330 - INFO - Processed datasets saved to ./processed_data/
2025-06-22 23:32:01,342 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-22 23:32:01,342 - INFO - Applying CPU optimizations for efficient training...
2025-06-22 23:32:01,342 - INFO - Disabled 4-bit quantization for CPU compatibility
2025-06-22 23:32:01,342 - INFO - CPU optimizations applied: batch_size=1, grad_accum=16, max_length=256, epochs=1
2025-06-22 23:32:01,342 - INFO - CPU detected: Using fp32 (most compatible)
2025-06-22 23:32:01,342 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-22 23:32:01,342 - INFO - CPU detected: Quantization disabled for compatibility
2025-06-22 23:32:06,310 - INFO - Using float32 for CPU
2025-06-23 20:16:52,114 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:16:52,114 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:16:52,114 - INFO - ==================================================
2025-06-23 20:16:52,114 - INFO - TRAINING PHASE
2025-06-23 20:16:52,114 - INFO - ==================================================
2025-06-23 20:16:52,114 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:16:57,898 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:16:57,906 - INFO - Limited dataset to 1000 samples
2025-06-23 20:16:57,906 - INFO - Formatting dataset samples...
2025-06-23 20:16:57,986 - INFO - Successfully formatted 1000 samples
2025-06-23 20:16:57,986 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:16:58,008 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:16:58,098 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:16:58,114 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-23 20:16:58,114 - INFO - Applying CPU optimizations for efficient training...
2025-06-23 20:16:58,114 - INFO - Disabled 4-bit quantization for CPU compatibility
2025-06-23 20:16:58,114 - INFO - CPU optimizations applied: batch_size=1, grad_accum=16, max_length=256, epochs=1
2025-06-23 20:16:58,114 - INFO - CPU detected: Using fp32 (most compatible)
2025-06-23 20:16:58,114 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:16:58,114 - INFO - CPU detected: Quantization disabled for compatibility
2025-06-23 20:16:58,494 - INFO - Using float32 for CPU
2025-06-23 20:19:48,089 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:19:48,089 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:19:48,089 - INFO - ==================================================
2025-06-23 20:19:48,089 - INFO - TRAINING PHASE
2025-06-23 20:19:48,089 - INFO - ==================================================
2025-06-23 20:19:48,089 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:19:53,025 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:19:53,037 - INFO - Limited dataset to 1000 samples
2025-06-23 20:19:53,037 - INFO - Formatting dataset samples...
2025-06-23 20:19:53,149 - INFO - Successfully formatted 1000 samples
2025-06-23 20:19:53,149 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:19:53,180 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:19:53,306 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:21:16,924 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:21:16,924 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:21:16,924 - INFO - ==================================================
2025-06-23 20:21:16,924 - INFO - TRAINING PHASE
2025-06-23 20:21:16,924 - INFO - ==================================================
2025-06-23 20:21:16,924 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:21:21,876 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:21:21,881 - INFO - Limited dataset to 1000 samples
2025-06-23 20:21:21,881 - INFO - Formatting dataset samples...
2025-06-23 20:21:21,972 - INFO - Successfully formatted 1000 samples
2025-06-23 20:21:21,975 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:21:21,988 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:21:22,085 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:22:41,046 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:22:41,046 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:22:41,046 - INFO - ==================================================
2025-06-23 20:22:41,046 - INFO - TRAINING PHASE
2025-06-23 20:22:41,046 - INFO - ==================================================
2025-06-23 20:22:41,046 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:22:45,591 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:22:45,591 - INFO - Limited dataset to 1000 samples
2025-06-23 20:22:45,591 - INFO - Formatting dataset samples...
2025-06-23 20:22:45,676 - INFO - Successfully formatted 1000 samples
2025-06-23 20:22:45,678 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:22:45,692 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:22:45,798 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:25:34,215 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:25:34,215 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:25:34,215 - INFO - ==================================================
2025-06-23 20:25:34,215 - INFO - TRAINING PHASE
2025-06-23 20:25:34,215 - INFO - ==================================================
2025-06-23 20:25:34,215 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:25:39,067 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:25:39,067 - INFO - Limited dataset to 1000 samples
2025-06-23 20:25:39,067 - INFO - Formatting dataset samples...
2025-06-23 20:25:39,144 - INFO - Successfully formatted 1000 samples
2025-06-23 20:25:39,144 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:25:39,160 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:25:39,270 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:25:39,270 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 20:25:39,270 - INFO - NVIDIA optimizations applied: batch_size=8, grad_accum=2, eval_steps=500, checkpointing=True
2025-06-23 20:25:39,270 - INFO - Using forced device: cuda
2025-06-23 20:25:39,270 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:25:39,270 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 20:25:39,686 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 20:25:40,507 - WARNING - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
2025-06-23 20:25:40,664 - WARNING - Failed to load model with quantization: 'frozenset' object has no attribute 'discard'
2025-06-23 20:25:40,664 - INFO - Retrying without quantization...
2025-06-23 20:25:57,353 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu and disk.
2025-06-23 20:25:57,353 - INFO - Model loaded successfully on cuda without quantization
2025-06-23 20:25:57,362 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 20:25:57,362 - INFO - Model and tokenizer loaded successfully
2025-06-23 20:25:57,362 - INFO - Setting up LoRA configuration...
2025-06-23 20:25:57,493 - INFO - LoRA applied successfully with configured modules: ['q_proj', 'v_proj']
2025-06-23 20:25:57,493 - INFO - LoRA configuration applied
2025-06-23 20:25:57,493 - INFO - Starting model training...
2025-06-23 20:25:57,507 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-23 20:25:57,507 - INFO - Applying CPU optimizations for efficient training...
2025-06-23 20:25:57,507 - INFO - Disabled 4-bit quantization for CPU compatibility
2025-06-23 20:25:57,507 - INFO - CPU optimizations applied: batch_size=1, grad_accum=16, max_length=256, epochs=1
2025-06-23 20:25:57,507 - INFO - CPU detected: Using fp32 (most compatible)
2025-06-23 20:25:57,513 - INFO - Training configuration: fp16=False, bf16=False, device=cpu
2025-06-23 20:28:48,957 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:28:48,957 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:28:48,957 - INFO - ==================================================
2025-06-23 20:28:48,957 - INFO - TRAINING PHASE
2025-06-23 20:28:48,958 - INFO - ==================================================
2025-06-23 20:28:48,958 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:28:53,553 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:28:53,557 - INFO - Limited dataset to 1000 samples
2025-06-23 20:28:53,557 - INFO - Formatting dataset samples...
2025-06-23 20:28:53,696 - INFO - Successfully formatted 1000 samples
2025-06-23 20:28:53,701 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:28:53,722 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:28:53,832 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:28:53,832 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 20:28:53,832 - INFO - NVIDIA optimizations applied: batch_size=8, grad_accum=2, eval_steps=500, checkpointing=True
2025-06-23 20:28:53,832 - INFO - Using forced device: cuda
2025-06-23 20:28:53,832 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:28:53,836 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 20:28:54,639 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 20:28:55,415 - WARNING - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
2025-06-23 20:28:55,544 - WARNING - Failed to load model with quantization: 'frozenset' object has no attribute 'discard'
2025-06-23 20:28:55,544 - INFO - Retrying without quantization...
2025-06-23 20:29:10,600 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu and disk.
2025-06-23 20:29:10,600 - INFO - Model loaded successfully on cuda without quantization
2025-06-23 20:29:10,615 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 20:29:10,615 - INFO - Model and tokenizer loaded successfully
2025-06-23 20:29:10,615 - INFO - Setting up LoRA configuration...
2025-06-23 20:29:10,743 - INFO - LoRA applied successfully with configured modules: ['q_proj', 'v_proj']
2025-06-23 20:29:10,744 - INFO - LoRA configuration applied
2025-06-23 20:29:10,744 - INFO - Starting model training...
2025-06-23 20:29:10,758 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-23 20:29:10,758 - INFO - Applying CPU optimizations for efficient training...
2025-06-23 20:29:10,758 - INFO - Disabled 4-bit quantization for CPU compatibility
2025-06-23 20:29:10,758 - INFO - CPU optimizations applied: batch_size=1, grad_accum=16, max_length=256, epochs=1
2025-06-23 20:29:10,758 - INFO - CPU detected: Using fp32 (most compatible)
2025-06-23 20:29:10,760 - INFO - Training configuration: fp16=False, bf16=False, device=cpu
2025-06-23 20:32:42,366 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:32:42,366 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:32:42,366 - INFO - ==================================================
2025-06-23 20:32:42,366 - INFO - TRAINING PHASE
2025-06-23 20:32:42,366 - INFO - ==================================================
2025-06-23 20:32:42,366 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:32:47,777 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:32:47,787 - INFO - Limited dataset to 1000 samples
2025-06-23 20:32:47,787 - INFO - Formatting dataset samples...
2025-06-23 20:32:48,056 - INFO - Successfully formatted 1000 samples
2025-06-23 20:32:48,064 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:32:48,105 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:32:48,390 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:32:48,394 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 20:32:48,394 - INFO - NVIDIA optimizations applied: batch_size=8, grad_accum=2, eval_steps=500, checkpointing=True
2025-06-23 20:32:48,394 - INFO - Using forced device: cuda
2025-06-23 20:32:48,394 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:32:48,397 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 20:32:48,854 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 20:32:50,127 - WARNING - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
2025-06-23 20:32:50,288 - WARNING - Failed to load model with quantization: 'frozenset' object has no attribute 'discard'
2025-06-23 20:32:50,288 - INFO - Retrying without quantization...
2025-06-23 20:33:10,522 - WARNING - Some parameters are on the meta device because they were offloaded to the disk and cpu.
2025-06-23 20:33:10,524 - INFO - Model loaded successfully on cuda without quantization
2025-06-23 20:33:10,526 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 20:33:10,526 - INFO - Model and tokenizer loaded successfully
2025-06-23 20:33:10,526 - INFO - Setting up LoRA configuration...
2025-06-23 20:33:10,684 - INFO - LoRA applied successfully with configured modules: ['q_proj', 'v_proj']
2025-06-23 20:33:10,688 - INFO - LoRA configuration applied
2025-06-23 20:33:10,688 - INFO - Starting model training...
2025-06-23 20:33:10,700 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-23 20:33:10,705 - INFO - Applying CPU optimizations for efficient training...
2025-06-23 20:33:10,705 - INFO - Disabled 4-bit quantization for CPU compatibility
2025-06-23 20:33:10,705 - INFO - CPU optimizations applied: batch_size=1, grad_accum=16, max_length=256, epochs=1
2025-06-23 20:33:10,705 - INFO - CPU detected: Using fp32 (most compatible)
2025-06-23 20:33:10,705 - INFO - Training configuration: fp16=False, bf16=False, device=cpu
2025-06-23 20:35:03,766 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:35:03,766 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:35:03,766 - INFO - ==================================================
2025-06-23 20:35:03,766 - INFO - TRAINING PHASE
2025-06-23 20:35:03,766 - INFO - ==================================================
2025-06-23 20:35:03,766 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:35:09,679 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:35:09,682 - INFO - Limited dataset to 1000 samples
2025-06-23 20:35:09,685 - INFO - Formatting dataset samples...
2025-06-23 20:35:09,789 - INFO - Successfully formatted 1000 samples
2025-06-23 20:35:09,794 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:35:09,814 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:35:09,958 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:35:09,958 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 20:35:09,958 - INFO - NVIDIA optimizations applied: batch_size=8, grad_accum=2, eval_steps=500, checkpointing=True
2025-06-23 20:35:09,958 - INFO - Using forced device: cuda
2025-06-23 20:35:09,958 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:35:09,962 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 20:35:10,891 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 20:35:11,823 - WARNING - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
2025-06-23 20:35:11,965 - WARNING - Failed to load model with quantization: 'frozenset' object has no attribute 'discard'
2025-06-23 20:35:11,965 - INFO - Retrying without quantization...
2025-06-23 20:35:35,065 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu and disk.
2025-06-23 20:35:35,067 - INFO - Model loaded successfully on cuda without quantization
2025-06-23 20:35:35,070 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 20:35:35,073 - INFO - Model and tokenizer loaded successfully
2025-06-23 20:35:35,073 - INFO - Setting up LoRA configuration...
2025-06-23 20:35:35,268 - INFO - LoRA applied successfully with configured modules: ['q_proj', 'v_proj']
2025-06-23 20:35:35,277 - INFO - LoRA configuration applied
2025-06-23 20:35:35,277 - INFO - Starting model training...
2025-06-23 20:35:35,294 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-23 20:35:35,294 - INFO - Applying CPU optimizations for efficient training...
2025-06-23 20:35:35,294 - INFO - Disabled 4-bit quantization for CPU compatibility
2025-06-23 20:35:35,294 - INFO - CPU optimizations applied: batch_size=1, grad_accum=16, max_length=256, epochs=1
2025-06-23 20:35:35,294 - INFO - CPU detected: Using fp32 (most compatible)
2025-06-23 20:35:35,297 - INFO - Training configuration: fp16=False, bf16=False, device=cpu
2025-06-23 20:40:41,041 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:40:41,041 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:40:41,041 - INFO - ==================================================
2025-06-23 20:40:41,041 - INFO - TRAINING PHASE
2025-06-23 20:40:41,041 - INFO - ==================================================
2025-06-23 20:40:41,041 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:40:46,085 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:40:46,089 - INFO - Limited dataset to 1000 samples
2025-06-23 20:40:46,089 - INFO - Formatting dataset samples...
2025-06-23 20:40:46,181 - INFO - Successfully formatted 1000 samples
2025-06-23 20:40:46,188 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:40:46,206 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:40:46,316 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:42:40,179 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 20:42:40,179 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:42:40,179 - INFO - ==================================================
2025-06-23 20:42:40,179 - INFO - TRAINING PHASE
2025-06-23 20:42:40,179 - INFO - ==================================================
2025-06-23 20:42:40,179 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 20:42:45,171 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 20:42:45,171 - INFO - Limited dataset to 1000 samples
2025-06-23 20:42:45,173 - INFO - Formatting dataset samples...
2025-06-23 20:42:45,252 - INFO - Successfully formatted 1000 samples
2025-06-23 20:42:45,258 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 20:42:45,271 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 20:42:45,427 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 20:42:45,427 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 20:42:45,427 - INFO - NVIDIA optimizations applied: batch_size=8, grad_accum=2, eval_steps=500, checkpointing=True
2025-06-23 20:42:45,427 - INFO - Using forced device: cuda
2025-06-23 20:42:45,427 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 20:42:45,430 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 20:42:46,243 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 20:42:47,223 - WARNING - The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.
2025-06-23 20:42:47,412 - WARNING - Failed to load model with quantization: 'frozenset' object has no attribute 'discard'
2025-06-23 20:42:47,415 - INFO - Retrying without quantization...
2025-06-23 20:44:07,130 - INFO - Model loaded successfully on cuda without quantization
2025-06-23 20:44:07,146 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 20:44:07,146 - INFO - Model and tokenizer loaded successfully
2025-06-23 20:44:07,146 - INFO - Setting up LoRA configuration...
2025-06-23 20:44:09,816 - INFO - LoRA applied successfully with configured modules: ['q_proj', 'v_proj']
2025-06-23 20:44:09,840 - INFO - LoRA configuration applied
2025-06-23 20:44:09,842 - INFO - Starting model training...
2025-06-23 20:44:09,876 - INFO - Using CPU for training: 16 cores, 16.5GB RAM
2025-06-23 20:44:09,877 - INFO - Applying CPU optimizations for efficient training...
2025-06-23 20:44:09,877 - INFO - Disabled 4-bit quantization for CPU compatibility
2025-06-23 20:44:09,877 - INFO - CPU optimizations applied: batch_size=1, grad_accum=16, max_length=256, epochs=1
2025-06-23 20:44:09,878 - INFO - CPU detected: Using fp32 (most compatible)
2025-06-23 20:44:09,972 - INFO - Training configuration: fp16=False, bf16=False, device=cpu
2025-06-23 21:29:14,896 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 21:29:14,898 - INFO - Using base model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 21:29:14,898 - INFO - ==================================================
2025-06-23 21:29:14,898 - INFO - TRAINING PHASE
2025-06-23 21:29:14,898 - INFO - ==================================================
2025-06-23 21:29:14,898 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 21:29:23,987 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 21:29:23,994 - INFO - Limited dataset to 1000 samples
2025-06-23 21:29:23,994 - INFO - Formatting dataset samples...
2025-06-23 21:29:24,426 - INFO - Successfully formatted 1000 samples
2025-06-23 21:29:24,433 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 21:29:24,474 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 21:29:24,825 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 21:29:24,870 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 21:29:24,874 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 21:29:24,874 - INFO - NVIDIA optimizations applied: batch_size=8, grad_accum=2, eval_steps=500, checkpointing=True
2025-06-23 21:29:24,874 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 21:29:24,876 - INFO - Loading model: codellama/CodeLlama-7b-Instruct-hf
2025-06-23 21:29:24,879 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 21:29:25,927 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 21:30:38,777 - INFO - Model loaded successfully on cuda with quantization
2025-06-23 21:30:38,790 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 21:30:38,792 - INFO - Model and tokenizer loaded successfully
2025-06-23 21:30:38,795 - INFO - Setting up LoRA configuration...
2025-06-23 21:30:39,136 - INFO - LoRA applied successfully with configured modules: ['q_proj', 'v_proj']
2025-06-23 21:30:39,144 - INFO - LoRA configuration applied
2025-06-23 21:30:39,147 - INFO - Starting model training...
2025-06-23 21:30:39,161 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 21:30:39,161 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 21:30:39,161 - INFO - NVIDIA optimizations applied: batch_size=8, grad_accum=1, eval_steps=500, checkpointing=True
2025-06-23 21:30:39,161 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 21:30:40,224 - INFO - Training configuration: fp16=False, bf16=True, device=cuda
2025-06-23 21:55:52,745 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 21:55:52,745 - INFO - Using base model: Salesforce/codet5p-770m
2025-06-23 21:55:52,745 - INFO - ==================================================
2025-06-23 21:55:52,745 - INFO - TRAINING PHASE
2025-06-23 21:55:52,745 - INFO - ==================================================
2025-06-23 21:55:52,745 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 21:55:58,089 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 21:55:58,093 - INFO - Limited dataset to 1000 samples
2025-06-23 21:55:58,093 - INFO - Formatting dataset samples...
2025-06-23 21:55:58,200 - INFO - Successfully formatted 1000 samples
2025-06-23 21:55:58,215 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 21:55:58,231 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 21:55:58,343 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 21:55:58,375 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 21:55:58,375 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 21:59:13,969 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 21:59:13,969 - INFO - Using base model: Salesforce/codet5p-770m
2025-06-23 21:59:13,969 - INFO - ==================================================
2025-06-23 21:59:13,969 - INFO - TRAINING PHASE
2025-06-23 21:59:13,969 - INFO - ==================================================
2025-06-23 21:59:13,969 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 21:59:19,340 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 21:59:19,340 - INFO - Limited dataset to 1000 samples
2025-06-23 21:59:19,340 - INFO - Formatting dataset samples...
2025-06-23 21:59:19,433 - INFO - Successfully formatted 1000 samples
2025-06-23 21:59:19,448 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 21:59:19,448 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 21:59:19,576 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 21:59:19,593 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 21:59:19,593 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 21:59:19,593 - INFO - NVIDIA optimizations applied: batch_size=1, grad_accum=2, eval_steps=1000, checkpointing=True
2025-06-23 21:59:19,593 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 21:59:19,593 - INFO - Loading model: Salesforce/codet5p-770m
2025-06-23 21:59:19,593 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 21:59:24,067 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 21:59:25,688 - WARNING - Failed to load model with quantization: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.
Model type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.
2025-06-23 21:59:25,688 - INFO - Retrying without quantization...
2025-06-23 22:04:34,342 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 22:04:34,342 - INFO - Using base model: Salesforce/codet5p-770m
2025-06-23 22:04:34,342 - INFO - ==================================================
2025-06-23 22:04:34,342 - INFO - TRAINING PHASE
2025-06-23 22:04:34,342 - INFO - ==================================================
2025-06-23 22:04:34,342 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 22:04:39,640 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 22:04:39,640 - INFO - Limited dataset to 1000 samples
2025-06-23 22:04:39,640 - INFO - Formatting dataset samples...
2025-06-23 22:04:39,736 - INFO - Successfully formatted 1000 samples
2025-06-23 22:04:39,736 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 22:04:39,752 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 22:04:39,863 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 22:04:39,892 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 22:04:39,892 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 22:04:39,892 - INFO - NVIDIA optimizations applied: batch_size=1, grad_accum=2, eval_steps=1000, checkpointing=True
2025-06-23 22:04:39,895 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 22:04:39,895 - INFO - Loading model: Salesforce/codet5p-770m
2025-06-23 22:04:39,895 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 22:04:40,615 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 22:06:00,065 - WARNING - Failed to load model with quantization: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.
See the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434
2025-06-23 22:06:00,065 - INFO - Retrying without quantization...
2025-06-23 22:17:38,745 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 22:17:38,749 - INFO - Using base model: Salesforce/codet5p-770m
2025-06-23 22:17:38,749 - INFO - ==================================================
2025-06-23 22:17:38,749 - INFO - TRAINING PHASE
2025-06-23 22:17:38,749 - INFO - ==================================================
2025-06-23 22:17:38,749 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 22:17:44,474 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 22:17:44,482 - INFO - Limited dataset to 1000 samples
2025-06-23 22:17:44,483 - INFO - Formatting dataset samples...
2025-06-23 22:17:44,792 - INFO - Successfully formatted 1000 samples
2025-06-23 22:17:44,803 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 22:17:44,844 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 22:17:45,069 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 22:17:45,118 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 22:17:45,118 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 22:17:45,118 - INFO - NVIDIA optimizations applied: batch_size=1, grad_accum=2, eval_steps=1000, checkpointing=True
2025-06-23 22:17:45,118 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 22:17:45,118 - INFO - Loading model: Salesforce/codet5p-770m
2025-06-23 22:17:45,122 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 22:17:46,091 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 22:23:25,772 - INFO - Model loaded successfully on cuda with quantization
2025-06-23 22:23:25,782 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 22:23:25,782 - INFO - Model and tokenizer loaded successfully
2025-06-23 22:23:25,784 - INFO - Setting up LoRA configuration...
2025-06-23 22:23:25,789 - WARNING - Configured target modules failed: ['q_proj', 'v_proj']
2025-06-23 22:23:25,789 - INFO - Auto-detecting target modules...
2025-06-23 22:23:25,789 - INFO - Auto-detecting target modules for LoRA...
2025-06-23 22:23:25,792 - INFO - Found target modules: ['k', 'q', 'o', 'v']
2025-06-23 22:23:26,265 - INFO - LoRA applied successfully with auto-detected modules: ['k', 'q', 'o', 'v']
2025-06-23 22:23:26,273 - INFO - LoRA configuration applied
2025-06-23 22:23:26,273 - INFO - Starting model training...
2025-06-23 22:23:26,273 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 22:23:26,273 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 22:23:26,273 - INFO - NVIDIA optimizations applied: batch_size=1, grad_accum=1, eval_steps=1000, checkpointing=True
2025-06-23 22:23:26,273 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 22:23:26,439 - INFO - Training configuration: fp16=False, bf16=True, device=cuda
2025-06-23 22:30:48,237 - INFO - Starting text-to-SQL model pipeline in full mode
2025-06-23 22:30:48,237 - INFO - Using base model: Salesforce/codet5p-770m
2025-06-23 22:30:48,237 - INFO - ==================================================
2025-06-23 22:30:48,237 - INFO - TRAINING PHASE
2025-06-23 22:30:48,237 - INFO - ==================================================
2025-06-23 22:30:48,237 - INFO - Loading dataset: gretelai/synthetic_text_to_sql
2025-06-23 22:30:54,259 - INFO - Dataset loaded successfully. Train size: 100000
2025-06-23 22:30:54,264 - INFO - Limited dataset to 1000 samples
2025-06-23 22:30:54,264 - INFO - Formatting dataset samples...
2025-06-23 22:30:54,515 - INFO - Successfully formatted 1000 samples
2025-06-23 22:30:54,519 - INFO - Used stratified split for balanced complexity distribution
2025-06-23 22:30:54,549 - INFO - Train samples: 900, Validation samples: 100
2025-06-23 22:30:54,835 - INFO - Processed datasets saved to ./processed_data/
2025-06-23 22:30:54,910 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 22:30:54,910 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 22:30:54,910 - INFO - NVIDIA optimizations applied: batch_size=1, grad_accum=2, eval_steps=1000, checkpointing=True
2025-06-23 22:30:54,911 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 22:30:54,911 - INFO - Loading model: Salesforce/codet5p-770m
2025-06-23 22:30:54,917 - INFO - 4-bit quantization enabled for cuda with torch.float16
2025-06-23 22:30:55,739 - INFO - Using float16 and auto device mapping for CUDA
2025-06-23 22:36:42,462 - INFO - Model loaded successfully on cuda with quantization
2025-06-23 22:36:42,473 - INFO - Gradient checkpointing enabled for memory optimization
2025-06-23 22:36:42,473 - INFO - Model and tokenizer loaded successfully
2025-06-23 22:36:42,473 - INFO - Setting up LoRA configuration...
2025-06-23 22:36:42,534 - WARNING - Configured target modules failed: ['q_proj', 'v_proj']
2025-06-23 22:36:42,536 - INFO - Auto-detecting target modules...
2025-06-23 22:36:42,536 - INFO - Auto-detecting target modules for LoRA...
2025-06-23 22:36:42,543 - INFO - Found target modules: ['k', 'o', 'v', 'q']
2025-06-23 22:36:43,351 - INFO - LoRA applied successfully with auto-detected modules: ['k', 'o', 'v', 'q']
2025-06-23 22:36:43,375 - INFO - LoRA configuration applied
2025-06-23 22:36:43,375 - INFO - Starting model training...
2025-06-23 22:36:43,375 - INFO - Detected NVIDIA GPU: NVIDIA GeForce RTX 3050 Laptop GPU (4.3GB)
2025-06-23 22:36:43,375 - INFO - Applying NVIDIA GPU optimizations for maximum performance...
2025-06-23 22:36:43,375 - INFO - NVIDIA optimizations applied: batch_size=1, grad_accum=1, eval_steps=1000, checkpointing=True
2025-06-23 22:36:43,379 - INFO - GPU supports bf16: Enabling bf16 for optimal training
2025-06-23 22:36:43,718 - INFO - Training configuration: fp16=False, bf16=True, device=cuda
2025-06-23 23:10:11,644 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
2025-06-23 23:33:09,874 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
2025-06-24 00:09:50,314 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
2025-06-24 00:45:51,560 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
2025-06-24 01:16:14,909 - INFO - The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.
2025-06-24 06:38:06,212 - INFO - Training completed. LoRA adapter saved to ./text-to-sql-final (adapter only)
2025-06-24 06:38:06,212 - INFO - Evaluating model...
2025-06-24 06:38:34,208 - INFO - ==================================================
2025-06-24 06:38:34,208 - INFO - DEPLOYMENT PHASE
2025-06-24 06:38:34,208 - INFO - ==================================================
2025-06-24 06:38:34,208 - INFO - Deploying model to Ollama...
2025-06-24 06:38:34,208 - INFO - Mapping Salesforce/codet5p-770m -> codellama:7b
2025-06-24 06:38:34,208 - INFO - Checking if base model codellama:7b is available...
2025-06-24 06:38:34,661 - INFO - Base model codellama:7b not found. Pulling from Ollama registry...
2025-06-24 06:41:47,256 - INFO - Successfully pulled codellama:7b
2025-06-24 06:41:47,256 - INFO - Creating Ollama Modelfile...
2025-06-24 06:41:47,256 - INFO - Mapping Salesforce/codet5p-770m -> codellama:7b
2025-06-24 06:41:47,268 - INFO - Modelfile created at ./Modelfile
2025-06-24 06:41:47,268 - INFO - Using Ollama base model: codellama:7b
2025-06-24 06:41:47,359 - INFO - Creating Ollama model with command: ollama create text-to-sql -f ./Modelfile
2025-06-24 06:41:47,464 - INFO - Successfully created Ollama model: text-to-sql
2025-06-24 06:41:47,468 - INFO -    ollama run text-to-sql
2025-06-24 06:41:47,468 - INFO - ==================================================
2025-06-24 06:41:47,468 - INFO - TESTING PHASE
2025-06-24 06:41:47,468 - INFO - ==================================================
2025-06-24 06:41:47,468 - INFO - Testing Ollama model...
2025-06-24 06:42:47,517 - ERROR - Error testing Ollama model: Command '['ollama', 'run', 'text-to-sql', 'CREATE TABLE users (id INT, name VARCHAR(50), email VARCHAR(100), created_at TIMESTAMP);\n\nQuery: Find all users created in the last 30 days']' timed out after 60 seconds
